# MNIST Classification using MLP (Deep Learning Case Study 1)

## ğŸ“Œ Objective
The objective of this case study is to implement a **Multi-Layer Perceptron (MLP)**
for handwritten digit classification using the **MNIST dataset**, and to analyze
model performance by applying different **variations and hyperparameters**.

---

## ğŸ“Š Dataset
- **Dataset:** MNIST Handwritten Digits
- **Image Size:** 28 Ã— 28 (grayscale)
- **Classes:** Digits 0â€“9
- **Total Samples:** 70,000  
  - Training: 60,000  
  - Testing: 10,000  

The dataset is loaded using TensorFlow Keras built-in utilities.

---

## âš™ï¸ Preprocessing Steps
- Reshaped images from `28Ã—28` to `784`
- Normalized pixel values to range `[0, 1]`
- Used train-test split provided by MNIST

---

## ğŸ§  Model Architecture (MLP)
- Input Layer: 784 neurons  
- Hidden Layer 1: 128 neurons  
- Hidden Layer 2: 64 neurons  
- Output Layer: 10 neurons  
- Activation Functions: ReLU / Sigmoid / Tanh (based on experiment)
- Output Activation: Softmax  

---

## ğŸ” Variations Implemented

### 1ï¸âƒ£ Activation Function Variations
- `relu.py`
- `sigmoid.py`
- `tanh.py`

**Purpose:**  
To analyze the effect of different activation functions on convergence speed and accuracy.

---

### 2ï¸âƒ£ Optimizer Variations
- `optimizer_adam.py`
- `optimizer_sgd.py`

**Purpose:**  
To compare optimization strategies and their impact on training efficiency.

---

### 3ï¸âƒ£ Epoch Variations
- `epochs_5.py`
- `epochs_10.py`
- `epochs_20.py`

**Purpose:**  
To observe underfitting, optimal fitting, and overfitting behavior.

---

## ğŸ“ Project Structure
